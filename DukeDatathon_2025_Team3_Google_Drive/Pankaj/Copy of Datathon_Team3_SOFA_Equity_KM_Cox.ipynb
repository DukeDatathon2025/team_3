{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install lifelines tableone"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUT1yOmZpupG","executionInfo":{"status":"ok","timestamp":1745758532452,"user_tz":240,"elapsed":23873,"user":{"displayName":"Pankaj Agarwal","userId":"13343705514209164174"}},"outputId":"74d02ba7-9931-47e4-c5a8-411e6717a43e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lifelines\n","  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n","Collecting tableone\n","  Downloading tableone-0.9.5-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.14.1)\n","Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n","Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.7.0)\n","Collecting autograd-gamma>=0.3 (from lifelines)\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting formulaic>=0.2.2 (from lifelines)\n","  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from tableone) (3.1.6)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from tableone) (0.9.0)\n","Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n","  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.13.2)\n","Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n","Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tableone-0.9.5-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=ec0fc27429853bd00a4ce27752c9529945a584b24890cdcc191a648d3a9f1d7d\n","  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n","Successfully built autograd-gamma\n","Installing collected packages: interface-meta, autograd-gamma, formulaic, tableone, lifelines\n","Successfully installed autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0 tableone-0.9.5\n"]}]},{"cell_type":"code","source":["# ==============================================================================\n","# === SETUP: Configuration & Dependencies =====================================\n","# ==============================================================================\n","print(\"--- Initializing Setup ---\")\n","\n","# --- Configuration ---\n","COHORT_CSV_PATH = '/content/drive/MyDrive/DukeDatathon_2025_Team3/Brian/files/df_cohort_v0.csv'\n","PATIENTS_CSV_PATH = '/content/drive/MyDrive/DukeDatathon_2025_Team3/Brian/files/patients.csv'\n","RESULTS_OUTPUT_DIR = '/content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/'\n","\n","# --- Dependencies ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, roc_curve\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","import pickle\n","import os\n","import traceback\n","import json\n","import matplotlib.pyplot as plt # For KM plots\n","import seaborn as sns # For KM plots styling\n","\n","try:\n","    from tableone import TableOne\n","    print(\"Successfully imported tableone.\")\n","except ImportError:\n","    print(\"INFO: Package 'tableone' not found. Install with: !pip install tableone\")\n","    TableOne = None\n","\n","try:\n","    from lifelines import KaplanMeierFitter, CoxPHFitter\n","    from lifelines.statistics import logrank_test\n","    print(\"Successfully imported lifelines.\")\n","except ImportError:\n","    print(\"INFO: Package 'lifelines' not found. Install with: !pip install lifelines\")\n","    KaplanMeierFitter, CoxPHFitter, logrank_test = None, None, None # Set to None\n","\n","# Google Drive specific\n","from google.colab import drive\n","\n","# --- Mount Google Drive ---\n","print(\"\\nAttempting to mount Google Drive...\")\n","try:\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Google Drive mounted successfully.\")\n","except Exception as e:\n","    print(f\"ERROR: Error mounting Google Drive: {e}\")\n","    raise SystemExit(\"Stopping: Cannot access Google Drive.\")\n","\n","# --- Create Output Directory ---\n","print(f\"\\nEnsuring results directory exists: {RESULTS_OUTPUT_DIR}\")\n","try:\n","    os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n","    print(\"Results directory ready.\")\n","except Exception as e:\n","    print(f\"ERROR: Error creating output directory: {e}\")\n","    print(\"Warning: Proceeding, but saving results might fail.\")\n","\n","# ==============================================================================\n","# === PART 1: DATA LOADING, PREPARATION & OUTCOME CREATION ====================\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"=== PART 1: DATA LOADING, PREPARATION & OUTCOME CREATION ===\")\n","print(\"=\"*60)\n","try:\n","    # Load the initial cohort created by Brian\n","    print(f\"Loading cohort data from: {COHORT_CSV_PATH}\")\n","    df_cohort = pd.read_csv(COHORT_CSV_PATH)\n","    print(f\"  Initial cohort shape: {df_cohort.shape}\")\n","\n","    # Load the patients table to get Date of Death (dod)\n","    print(f\"Loading patients data from: {PATIENTS_CSV_PATH}\")\n","    df_patients = pd.read_csv(PATIENTS_CSV_PATH)\n","\n","    # Convert relevant time columns to datetime objects\n","    print(\"\\nConverting time columns...\")\n","    time_cols_cohort = ['intime', 'deathtime']\n","    for col in time_cols_cohort:\n","        if col in df_cohort.columns:\n","             df_cohort[col] = pd.to_datetime(df_cohort[col], errors='coerce')\n","             print(f\"  Converted '{col}' in cohort.\")\n","\n","    # --- Merge Date of Death (dod) ---\n","    print(\"\\nMerging Date of Death (dod)...\")\n","    df_cohort = pd.merge(df_cohort, df_patients[['subject_id', 'dod']], on='subject_id', how='left')\n","    df_cohort['dod'] = pd.to_datetime(df_cohort['dod'], errors='coerce')\n","    print(f\"  Cohort shape after merging 'dod': {df_cohort.shape}\")\n","    print(\"  Merged 'dod' successfully.\")\n","\n","    # --- Create the Accurate 30-Day Mortality Outcome ---\n","    print(\"\\nCreating 'mortality_30day' outcome...\")\n","    time_diff_days = (df_cohort['dod'] - df_cohort['intime']).dt.days\n","    df_cohort['mortality_30day'] = 0\n","    df_cohort.loc[(time_diff_days >= 0) & (time_diff_days <= 30), 'mortality_30day'] = 1\n","    print(\"  Set mortality based on 'dod' within 30 days.\")\n","    if 'hospital_expire_flag' in df_cohort.columns:\n","        initial_deaths = df_cohort['mortality_30day'].sum()\n","        df_cohort.loc[df_cohort['hospital_expire_flag'] == 1, 'mortality_30day'] = 1\n","        final_deaths = df_cohort['mortality_30day'].sum()\n","        print(f\"  Refined mortality using 'hospital_expire_flag' (added {final_deaths - initial_deaths} potential events).\")\n","    else: print(\"  Warning: 'hospital_expire_flag' not found.\")\n","    print(\"  'mortality_30day' column created.\")\n","    print(\"-\" * 30)\n","\n","    # --- Prepare Features: Renaming, Cleaning, Imputation ---\n","    print(\"\\nPreparing Features...\")\n","    if 'race' in df_cohort.columns and 'ethnicity' not in df_cohort.columns:\n","        df_cohort.rename(columns={'race': 'ethnicity'}, inplace=True)\n","        print(\"  Renamed 'race' to 'ethnicity'.\")\n","        race_col = 'ethnicity'\n","    elif 'ethnicity' in df_cohort.columns: race_col = 'ethnicity'\n","    elif 'race' in df_cohort.columns: race_col = 'race'\n","    else: race_col = None\n","\n","    sdoh_cols_to_clean = [col for col in [race_col, 'insurance', 'language'] if col is not None and col in df_cohort.columns]\n","    print(f\"  Cleaning SDoH columns: {sdoh_cols_to_clean}\")\n","    for col in sdoh_cols_to_clean:\n","        df_cohort[col] = df_cohort[col].fillna('UNKNOWN').astype(str)\n","        value_counts = df_cohort[col].value_counts(normalize=True)\n","        rare_cats = value_counts[value_counts < 0.01].index\n","        rare_cats = [cat for cat in rare_cats if cat not in ['UNKNOWN', 'OTHER']]\n","        if rare_cats: df_cohort[col] = df_cohort[col].replace(rare_cats, 'OTHER')\n","\n","    predictor_cols_to_impute = ['age', 'sofa', 'charlson_comorbidity_index']\n","    print(f\"\\n  Imputing missing values in predictors: {predictor_cols_to_impute}\")\n","    for col in predictor_cols_to_impute:\n","        if col in df_cohort.columns:\n","            if df_cohort[col].isnull().any():\n","                num_missing = df_cohort[col].isnull().sum()\n","                median_val = df_cohort[col].median()\n","                df_cohort[col].fillna(median_val, inplace=True)\n","                print(f\"    Imputed {num_missing} missing '{col}' with median ({median_val}).\")\n","        else: print(f\"    Warning: Predictor '{col}' not found.\")\n","    print(\"\\nData Preparation Complete.\")\n","\n","except Exception as e:\n","    print(f\"ERROR during Data Loading/Preparation: {e}\")\n","    traceback.print_exc()\n","    raise SystemExit(\"Stopping due to critical error in Data Loading/Preparation.\")\n","\n","\n","# ==============================================================================\n","# === PART 2: EXPLORATORY DATA ANALYSIS (EDA) & TABLEONE ======================\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"=== PART 2: EXPLORATORY DATA ANALYSIS (EDA) & TABLEONE ===\")\n","print(\"=\"*60)\n","eda_results = {} # Initialize dictionary to store summaries\n","try:\n","    print(\"Calculating Overall EDA summaries...\")\n","    eda_results['total_stays'] = int(df_cohort['stay_id'].nunique())\n","    eda_results['total_patients'] = int(df_cohort['subject_id'].nunique())\n","    eda_results['overall_mortality_rate_30day'] = float(df_cohort['mortality_30day'].mean())\n","    print(f\"  Overall Metrics: Stays={eda_results['total_stays']}, Patients={eda_results['total_patients']}, Mortality={eda_results['overall_mortality_rate_30day']:.3f}\")\n","\n","    if TableOne:\n","        print(\"\\nGenerating TableOne summaries...\")\n","        tableone_cols = ['age', 'gender', race_col, 'insurance', 'language', 'sofa', 'charlson_comorbidity_index']\n","        tableone_categorical = ['gender', race_col, 'insurance', 'language']\n","        tableone_nonnormal = ['age', 'sofa', 'charlson_comorbidity_index']\n","        tableone_cols = [col for col in tableone_cols if col in df_cohort.columns]\n","        tableone_categorical = [col for col in tableone_categorical if col in df_cohort.columns]\n","        tableone_nonnormal = [col for col in tableone_nonnormal if col in df_cohort.columns]\n","\n","        try:\n","            print(\"  Calculating Overall Table 1...\")\n","            table1_overall = TableOne(df_cohort, columns=tableone_cols, categorical=tableone_categorical, nonnormal=tableone_nonnormal, pval=False, htest_name=False)\n","            table1_overall_file = os.path.join(RESULTS_OUTPUT_DIR, 'table1_overall.csv')\n","            table1_overall.to_csv(table1_overall_file)\n","            print(f\"  Overall Table 1 saved to {table1_overall_file}\")\n","        except Exception as e: print(f\"  ERROR generating Overall Table 1: {e}\")\n","\n","        try:\n","            print(\"\\n  Calculating Table 1 stratified by 30-Day Mortality...\")\n","            table1_strat_mort = TableOne(df_cohort, columns=tableone_cols, categorical=tableone_categorical, nonnormal=tableone_nonnormal, groupby='mortality_30day', pval=True, htest_name=True)\n","            table1_strat_mort_file = os.path.join(RESULTS_OUTPUT_DIR, 'table1_stratified_mortality.csv')\n","            table1_strat_mort.to_csv(table1_strat_mort_file)\n","            print(f\"  Stratified Table 1 (Mortality) saved to {table1_strat_mort_file}\")\n","        except Exception as e: print(f\"  ERROR generating Mortality-Stratified Table 1: {e}\")\n","\n","        if race_col and race_col in df_cohort.columns:\n","            try:\n","                print(f\"\\n  Calculating Table 1 stratified by {race_col}...\")\n","                table1_strat_sdoh = TableOne(df_cohort, columns=tableone_cols, categorical=tableone_categorical, nonnormal=tableone_nonnormal, groupby=race_col, pval=False, htest_name=False)\n","                table1_strat_sdoh_file = os.path.join(RESULTS_OUTPUT_DIR, f'table1_stratified_{race_col}.csv')\n","                table1_strat_sdoh.to_csv(table1_strat_sdoh_file)\n","                print(f\"  Stratified Table 1 ({race_col}) saved to {table1_strat_sdoh_file}\")\n","            except Exception as e: print(f\"  ERROR generating {race_col}-Stratified Table 1: {e}\")\n","        else: print(f\"  Skipping Table 1 stratified by ethnicity/race as column '{race_col}' was not found/valid.\")\n","    else:\n","        print(\"\\nPackage 'tableone' not installed. Skipping detailed Table 1 generation.\")\n","        print(\"  Calculating basic summaries instead...\")\n","        numeric_summary = df_cohort[[col for col in ['age', 'sofa', 'charlson_comorbidity_index'] if col in df_cohort.columns]].describe().to_dict()\n","        eda_results['numeric_summary'] = numeric_summary\n","        categorical_counts = {}\n","        for col in ['gender'] + sdoh_cols_to_clean:\n","            if col in df_cohort.columns: categorical_counts[col] = df_cohort[col].value_counts().to_dict()\n","        eda_results['categorical_counts'] = categorical_counts\n","        print(\"  Basic summaries calculated.\")\n","\n","    eda_results_file = os.path.join(RESULTS_OUTPUT_DIR, 'eda_results.pkl')\n","    with open(eda_results_file, 'wb') as f: pickle.dump(eda_results, f)\n","    print(f\"\\nBasic EDA results dictionary saved to: {eda_results_file}\")\n","except Exception as e:\n","    print(f\"An error occurred during EDA: {e}\")\n","    traceback.print_exc()\n","\n","\n","# ==============================================================================\n","# === PART 3: EQUITY ANALYSIS (Stratified Logistic Regression) ================\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"=== PART 3: EQUITY ANALYSIS (Stratified Logistic Regression) ===\")\n","print(\"=\"*60)\n","target = 'mortality_30day'\n","numeric_features = ['age', 'sofa', 'charlson_comorbidity_index']\n","categorical_features = ['gender']\n","sdoh_factors = [col for col in [race_col, 'insurance', 'language'] if col is not None and col in df_cohort.columns]\n","\n","analysis_needed_cols = [target] + numeric_features + categorical_features + sdoh_factors\n","missing_analysis_cols = [col for col in analysis_needed_cols if col not in df_cohort.columns]\n","if missing_analysis_cols:\n","    print(f\"FATAL ERROR: Cannot proceed. Missing columns: {missing_analysis_cols}\")\n","    raise SystemExit(\"Stopping due to missing columns for analysis.\")\n","\n","print(f\"Starting Stratified Analysis using SDoH factors: {sdoh_factors}\")\n","results_auc = {}\n","results_roc = {}\n","try:\n","    for factor in sdoh_factors:\n","        print(f\"\\n--- Analyzing Factor: {factor} ---\")\n","        results_auc[factor] = {}\n","        results_roc[factor] = {}\n","        categories = df_cohort[factor].unique()\n","        for category in categories:\n","            if pd.isna(category): continue\n","            print(f\"  Processing Category: {category}\")\n","            df_group = df_cohort[df_cohort[factor] == category].copy()\n","            if len(df_group) < 50 or df_group[target].nunique() < 2:\n","                print(f\"    Skipping category '{category}' (data={len(df_group)}, outcomes={df_group[target].nunique()})\")\n","                continue\n","            current_numeric_features = [f for f in numeric_features if f in df_group.columns]\n","            current_categorical_features = [f for f in categorical_features if f in df_group.columns]\n","            features_to_use = current_numeric_features + current_categorical_features\n","            X = df_group[features_to_use]\n","            y = df_group[target]\n","            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n","            transformers = []\n","            if current_numeric_features: transformers.append(('num', StandardScaler(), current_numeric_features))\n","            if current_categorical_features: transformers.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), current_categorical_features))\n","            if not transformers: continue\n","            preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n","            model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))])\n","            try:\n","                model_pipeline.fit(X_train, y_train)\n","                y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]\n","                auc = roc_auc_score(y_test, y_pred_proba)\n","                results_auc[factor][category] = auc\n","                fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n","                results_roc[factor][category] = {'fpr': fpr.tolist(), 'tpr': tpr.tolist(), 'auc': auc}\n","                print(f\"    Category: {category} | Test AUC: {auc:.4f}\")\n","            except Exception as model_e: print(f\"    ERROR processing category '{category}': {model_e}\")\n","    print(\"\\n--- Equity Analysis Finished ---\")\n","    print(\"\\nAUC Results Summary:\")\n","    print(json.dumps(results_auc, indent=2, default=str))\n","    print(\"\\nSaving analysis results...\")\n","    auc_results_file = os.path.join(RESULTS_OUTPUT_DIR, 'equity_auc_results.pkl')\n","    roc_results_file = os.path.join(RESULTS_OUTPUT_DIR, 'equity_roc_results.pkl')\n","    with open(auc_results_file, 'wb') as f: pickle.dump(results_auc, f)\n","    print(f\"  AUC results saved successfully to: {auc_results_file}\")\n","    with open(roc_results_file, 'wb') as f: pickle.dump(results_roc, f)\n","    print(f\"  ROC results saved successfully to: {roc_results_file}\")\n","except Exception as e:\n","    print(f\"An critical error occurred during the Equity Analysis phase: {e}\")\n","    traceback.print_exc()\n","\n","\n","# ==============================================================================\n","# === PART 4: SURVIVAL ANALYSIS PREPARATION ===================================\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"=== PART 4: SURVIVAL ANALYSIS PREPARATION ===\")\n","print(\"=\"*60)\n","\n","# This part calculates the duration variable needed for KM and Cox\n","try:\n","    print(\"Calculating time-to-event ('duration_30day') for survival analysis...\")\n","    # Ensure intime and dod are datetime objects (might be redundant but safe)\n","    df_cohort['intime'] = pd.to_datetime(df_cohort['intime'], errors='coerce')\n","    df_cohort['dod'] = pd.to_datetime(df_cohort['dod'], errors='coerce')\n","\n","    # Time difference in days (using total_seconds for precision)\n","    time_diff = (df_cohort['dod'] - df_cohort['intime']).dt.total_seconds() / (60*60*24)\n","\n","    # Calculate duration: time to death if < 30 days, otherwise 30 days (censoring time)\n","    # Censor at 30 days if dod is null OR if death occurred after 30 days\n","    df_cohort['duration_30day'] = np.where(\n","        pd.notna(time_diff) & (time_diff <= 30) & (time_diff >= 0),\n","        time_diff, # Use actual time difference if death within 30 days\n","        30.0       # Censor at 30 days otherwise\n","    )\n","    # Ensure duration is non-negative\n","    df_cohort['duration_30day'] = df_cohort['duration_30day'].clip(lower=0.001) # Use small positive floor for stability\n","\n","    print(\"  Created 'duration_30day' column.\")\n","    print(\"  Sample of duration and mortality:\")\n","    print(df_cohort[['intime', 'dod', 'mortality_30day', 'duration_30day']].head())\n","    print(\"\\n  Summary statistics for 'duration_30day':\")\n","    print(df_cohort['duration_30day'].describe())\n","\n","except Exception as e:\n","    print(f\"ERROR during Survival Analysis Preparation: {e}\")\n","    traceback.print_exc()\n","    print(\"Warning: Survival prep failed. Subsequent survival analyses might fail.\")\n","\n","# ==============================================================================\n","# === PART 5: SURVIVAL ANALYSIS (Kaplan-Meier & Cox) ==========================\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"=== PART 5: SURVIVAL ANALYSIS (Kaplan-Meier & Cox) ===\")\n","print(\"=\"*60)\n","\n","# Check if lifelines is available and data is ready\n","if KaplanMeierFitter is None or CoxPHFitter is None:\n","    print(\"Skipping Survival Analysis because 'lifelines' package is not installed.\")\n","elif 'duration_30day' not in df_cohort.columns or 'mortality_30day' not in df_cohort.columns:\n","     print(\"Skipping Survival Analysis because 'duration_30day' or 'mortality_30day' column is missing.\")\n","else:\n","    # --- Kaplan-Meier Analysis ---\n","    print(\"\\n--- Kaplan-Meier Survival Analysis ---\")\n","    km_results = {} # To store KM plots/data if needed for Streamlit (optional)\n","\n","    # Define duration and event columns\n","    duration_col = 'duration_30day'\n","    event_col = 'mortality_30day'\n","\n","    # Plot KM curves stratified by SDoH factors\n","    for factor in sdoh_factors:\n","        print(f\"\\n  Generating KM plot stratified by '{factor}'...\")\n","        plt.figure(figsize=(10, 6))\n","        ax = plt.gca() # Get current axes\n","\n","        categories = sorted(df_cohort[factor].unique()) # Sort categories for consistent legend order\n","\n","        # Store results for pairwise logrank tests\n","        logrank_results = {}\n","\n","        for i, category in enumerate(categories):\n","            # Filter data for the category\n","            df_group = df_cohort[df_cohort[factor] == category]\n","\n","            if len(df_group) < 20: # Need sufficient data for KM curve\n","                 print(f\"    Skipping KM for category '{category}' in factor '{factor}' (too few samples: {len(df_group)})\")\n","                 continue\n","\n","            # Fit Kaplan-Meier estimator\n","            kmf = KaplanMeierFitter()\n","            kmf.fit(df_group[duration_col], event_observed=df_group[event_col], label=f\"{category} (n={len(df_group)})\")\n","\n","            # Plot the curve\n","            kmf.plot_survival_function(ax=ax, ci_show=False) # ci_show=False for cleaner multi-group plot\n","\n","            # Compare with the reference group (e.g., the first category) using logrank test\n","            if i > 0:\n","                 ref_group = df_cohort[df_cohort[factor] == categories[0]]\n","                 try:\n","                     results = logrank_test(ref_group[duration_col], df_group[duration_col],\n","                                            ref_group[event_col], df_group[event_col])\n","                     logrank_results[f\"{categories[0]} vs {category}\"] = results.p_value\n","                     print(f\"    Log-rank test ({categories[0]} vs {category}): p-value = {results.p_value:.4f}\")\n","                 except Exception as lr_e:\n","                      print(f\"    Could not perform logrank test for {category}: {lr_e}\")\n","\n","\n","        # Customize and save the plot\n","        plt.title(f'30-Day Survival Stratified by {factor.replace(\"_\", \" \").title()}')\n","        plt.xlabel('Days Since ICU Admission')\n","        plt.ylabel('Survival Probability')\n","        plt.ylim(0, 1)\n","        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","        plt.legend(title=factor.replace(\"_\", \" \").title())\n","        plt.tight_layout()\n","        km_plot_file = os.path.join(RESULTS_OUTPUT_DIR, f'km_plot_{factor}.png')\n","        try:\n","            plt.savefig(km_plot_file, dpi=150)\n","            print(f\"  KM plot saved to {km_plot_file}\")\n","            plt.close() # Close the plot to avoid displaying it inline here\n","            km_results[f'km_plot_{factor}'] = km_plot_file # Store path for potential Streamlit use\n","            km_results[f'logrank_{factor}'] = logrank_results # Store p-values\n","        except Exception as save_e:\n","            print(f\"  ERROR saving KM plot for {factor}: {save_e}\")\n","            plt.close()\n","\n","\n","    # --- Cox Proportional Hazards Analysis ---\n","    print(\"\\n--- Cox Proportional Hazards Analysis ---\")\n","    cox_results = {} # To store Cox model summaries\n","\n","    # Prepare data for Cox model - requires dummy variables for categoricals\n","    print(\"  Preparing data for Cox model (dummy variables)...\")\n","    try:\n","        # Select columns needed\n","        cox_cols = [duration_col, event_col, 'age', 'sofa', 'charlson_comorbidity_index', 'gender'] + sdoh_factors\n","        df_cox = df_cohort[cox_cols].copy()\n","\n","        # Create dummy variables for ALL categorical predictors\n","        # drop_first=True to avoid multicollinearity\n","        categorical_for_cox = ['gender'] + sdoh_factors\n","        df_cox_dummies = pd.get_dummies(df_cox, columns=categorical_for_cox, drop_first=True, dummy_na=False) # dummy_na=False ignores NaNs created by cleaning\n","        print(f\"  Data prepared for Cox model with {df_cox_dummies.shape[1]} columns.\")\n","        # print(\"  Columns for Cox:\", df_cox_dummies.columns.tolist()) # Uncomment to check columns\n","\n","        # 1. Main Effects Model\n","        print(\"\\n  Fitting Cox Main Effects Model...\")\n","        cph_main = CoxPHFitter(penalizer=0.01) # Small penalizer for stability\n","        # Ensure all column names are strings and valid identifiers if necessary\n","        df_cox_dummies.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_cox_dummies.columns]\n","        # Re-define duration and event cols in case names were changed\n","        duration_col_cox = \"\".join (c if c.isalnum() else \"_\" for c in str(duration_col))\n","        event_col_cox = \"\".join (c if c.isalnum() else \"_\" for c in str(event_col))\n","        cph_main.fit(df_cox_dummies, duration_col=duration_col_cox, event_col=event_col_cox)\n","        print(\"  Main Effects Model Summary:\")\n","        cph_main.print_summary(decimals=3, style='ascii')\n","        cox_results['main_summary_df'] = cph_main.summary\n","\n","        # Save main model summary\n","        main_cox_file = os.path.join(RESULTS_OUTPUT_DIR, 'cox_model_main_summary.csv')\n","        cph_main.summary.to_csv(main_cox_file)\n","        print(f\"  Main Cox model summary saved to {main_cox_file}\")\n","\n","        # 2. Interaction Model (SOFA * SDoH) - More Complex\n","        print(\"\\n  Fitting Cox Interaction Model (SOFA * SDoH)...\")\n","        # Need to define interaction terms carefully\n","        # We focus on SOFA interactions with the *reference* dummy variables\n","        df_interaction = df_cox_dummies.copy()\n","        sofa_col_cox = \"\".join (c if c.isalnum() else \"_\" for c in str('sofa')) # Get cleaned sofa column name\n","\n","        interaction_terms = []\n","        base_formula_cols = [col for col in df_interaction.columns if col not in [duration_col_cox, event_col_cox]]\n","\n","        # Create interaction terms manually\n","        for factor in sdoh_factors:\n","             # Get dummy columns related to this factor (excluding the dropped reference level)\n","             dummy_cols = [col for col in df_interaction.columns if col.startswith(factor + \"_\")]\n","             for dummy_col in dummy_cols:\n","                  interaction_name = f\"{sofa_col_cox}_x_{dummy_col}\"\n","                  df_interaction[interaction_name] = df_interaction[sofa_col_cox] * df_interaction[dummy_col]\n","                  interaction_terms.append(interaction_name)\n","\n","        if interaction_terms:\n","            print(f\"  Created {len(interaction_terms)} interaction terms for SOFA.\")\n","            # Fit model including interaction terms\n","            cph_interact = CoxPHFitter(penalizer=0.01)\n","            formula_cols = base_formula_cols + interaction_terms\n","            # Ensure no duplicates\n","            formula_cols = list(dict.fromkeys(formula_cols))\n","\n","            cph_interact.fit(df_interaction[[duration_col_cox, event_col_cox] + formula_cols], duration_col=duration_col_cox, event_col=event_col_cox)\n","            print(\"\\n  Interaction Model Summary:\")\n","            cph_interact.print_summary(decimals=3, style='ascii')\n","            cox_results['interaction_summary_df'] = cph_interact.summary\n","\n","            # Save interaction model summary\n","            interact_cox_file = os.path.join(RESULTS_OUTPUT_DIR, 'cox_model_interaction_summary.csv')\n","            cph_interact.summary.to_csv(interact_cox_file)\n","            print(f\"  Interaction Cox model summary saved to {interact_cox_file}\")\n","        else:\n","            print(\"  No interaction terms were created (possibly due to missing SDoH dummies). Skipping interaction model.\")\n","\n","    except Exception as cox_e:\n","        print(f\"ERROR during Cox analysis: {cox_e}\")\n","        traceback.print_exc()\n","\n","\n","    # --- Save Survival Analysis Results (optional pickle of summaries/plot paths) ---\n","    survival_results_file = os.path.join(RESULTS_OUTPUT_DIR, 'survival_analysis_results.pkl')\n","    survival_save_data = {'km_results': km_results, 'cox_results': cox_results}\n","    with open(survival_results_file, 'wb') as f:\n","        pickle.dump(survival_save_data, f)\n","    print(f\"\\nSurvival analysis results (plot paths, Cox summaries) saved to: {survival_results_file}\")\n","\n","\n","# ==============================================================================\n","# === SCRIPT EXECUTION COMPLETE ================================================\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"=== SCRIPT EXECUTION COMPLETE ===\")\n","print(\"=\"*60)\n","print(f\"Check the directory '{RESULTS_OUTPUT_DIR}' for analysis outputs.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hp97PKWGjCPq","executionInfo":{"status":"ok","timestamp":1745758596071,"user_tz":240,"elapsed":50529,"user":{"displayName":"Pankaj Agarwal","userId":"13343705514209164174"}},"outputId":"7547e6cd-e430-48f0-e600-dae0a11867e4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Initializing Setup ---\n","Successfully imported tableone.\n","Successfully imported lifelines.\n","\n","Attempting to mount Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully.\n","\n","Ensuring results directory exists: /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/\n","Results directory ready.\n","\n","============================================================\n","=== PART 1: DATA LOADING, PREPARATION & OUTCOME CREATION ===\n","============================================================\n","Loading cohort data from: /content/drive/MyDrive/DukeDatathon_2025_Team3/Brian/files/df_cohort_v0.csv\n","  Initial cohort shape: (32899, 14)\n","Loading patients data from: /content/drive/MyDrive/DukeDatathon_2025_Team3/Brian/files/patients.csv\n","\n","Converting time columns...\n","  Converted 'intime' in cohort.\n","  Converted 'deathtime' in cohort.\n","\n","Merging Date of Death (dod)...\n","  Cohort shape after merging 'dod': (32899, 15)\n","  Merged 'dod' successfully.\n","\n","Creating 'mortality_30day' outcome...\n","  Set mortality based on 'dod' within 30 days.\n","  Refined mortality using 'hospital_expire_flag' (added 508 potential events).\n","  'mortality_30day' column created.\n","------------------------------\n","\n","Preparing Features...\n","  Renamed 'race' to 'ethnicity'.\n","  Cleaning SDoH columns: ['ethnicity', 'insurance', 'language']\n","\n","  Imputing missing values in predictors: ['age', 'sofa', 'charlson_comorbidity_index']\n","\n","Data Preparation Complete.\n","\n","============================================================\n","=== PART 2: EXPLORATORY DATA ANALYSIS (EDA) & TABLEONE ===\n","============================================================\n","Calculating Overall EDA summaries...\n","  Overall Metrics: Stays=32899, Patients=25570, Mortality=0.213\n","\n","Generating TableOne summaries...\n","  Calculating Overall Table 1...\n","  Overall Table 1 saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/table1_overall.csv\n","\n","  Calculating Table 1 stratified by 30-Day Mortality...\n","  Stratified Table 1 (Mortality) saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/table1_stratified_mortality.csv\n","\n","  Calculating Table 1 stratified by ethnicity...\n","  Stratified Table 1 (ethnicity) saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/table1_stratified_ethnicity.csv\n","\n","Basic EDA results dictionary saved to: /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/eda_results.pkl\n","\n","============================================================\n","=== PART 3: EQUITY ANALYSIS (Stratified Logistic Regression) ===\n","============================================================\n","Starting Stratified Analysis using SDoH factors: ['ethnicity', 'insurance', 'language']\n","\n","--- Analyzing Factor: ethnicity ---\n","  Processing Category: BLACK/AFRICAN AMERICAN\n","    Category: BLACK/AFRICAN AMERICAN | Test AUC: 0.7638\n","  Processing Category: OTHER\n","    Category: OTHER | Test AUC: 0.7553\n","  Processing Category: WHITE\n","    Category: WHITE | Test AUC: 0.7415\n","  Processing Category: UNKNOWN\n","    Category: UNKNOWN | Test AUC: 0.7322\n","  Processing Category: UNABLE TO OBTAIN\n","    Category: UNABLE TO OBTAIN | Test AUC: 0.8146\n","  Processing Category: WHITE - OTHER EUROPEAN\n","    Category: WHITE - OTHER EUROPEAN | Test AUC: 0.7270\n","  Processing Category: ASIAN\n","    Category: ASIAN | Test AUC: 0.7192\n","  Processing Category: HISPANIC/LATINO - PUERTO RICAN\n","    Category: HISPANIC/LATINO - PUERTO RICAN | Test AUC: 0.7417\n","  Processing Category: WHITE - RUSSIAN\n","    Category: WHITE - RUSSIAN | Test AUC: 0.7888\n","  Processing Category: ASIAN - CHINESE\n","    Category: ASIAN - CHINESE | Test AUC: 0.8641\n","\n","--- Analyzing Factor: insurance ---\n","  Processing Category: Medicare\n","    Category: Medicare | Test AUC: 0.7287\n","  Processing Category: Private\n","    Category: Private | Test AUC: 0.8040\n","  Processing Category: Medicaid\n","    Category: Medicaid | Test AUC: 0.7390\n","  Processing Category: Other\n","    Category: Other | Test AUC: 0.7530\n","  Processing Category: UNKNOWN\n","    Category: UNKNOWN | Test AUC: 0.8325\n","  Processing Category: OTHER\n","    Skipping category 'OTHER' (data=1, outcomes=1)\n","\n","--- Analyzing Factor: language ---\n","  Processing Category: English\n","    Category: English | Test AUC: 0.7460\n","  Processing Category: OTHER\n","    Category: OTHER | Test AUC: 0.6922\n","  Processing Category: Spanish\n","    Category: Spanish | Test AUC: 0.7800\n","  Processing Category: UNKNOWN\n","    Category: UNKNOWN | Test AUC: 0.8110\n","  Processing Category: Russian\n","    Category: Russian | Test AUC: 0.7526\n","  Processing Category: Chinese\n","    Category: Chinese | Test AUC: 0.6133\n","\n","--- Equity Analysis Finished ---\n","\n","AUC Results Summary:\n","{\n","  \"ethnicity\": {\n","    \"BLACK/AFRICAN AMERICAN\": 0.7638237045860632,\n","    \"OTHER\": 0.7552872474747475,\n","    \"WHITE\": 0.7414954527902424,\n","    \"UNKNOWN\": 0.732207735074825,\n","    \"UNABLE TO OBTAIN\": 0.8146341463414635,\n","    \"WHITE - OTHER EUROPEAN\": 0.7269696969696969,\n","    \"ASIAN\": 0.7192028985507246,\n","    \"HISPANIC/LATINO - PUERTO RICAN\": 0.7417380660954712,\n","    \"WHITE - RUSSIAN\": 0.7887767969735182,\n","    \"ASIAN - CHINESE\": 0.8640625\n","  },\n","  \"insurance\": {\n","    \"Medicare\": 0.728677294409831,\n","    \"Private\": 0.8040151808664425,\n","    \"Medicaid\": 0.7389645706662562,\n","    \"Other\": 0.7530360531309298,\n","    \"UNKNOWN\": 0.832520325203252\n","  },\n","  \"language\": {\n","    \"English\": 0.7459850442053648,\n","    \"OTHER\": 0.6922310756972112,\n","    \"Spanish\": 0.7799956130730423,\n","    \"UNKNOWN\": 0.8110119047619048,\n","    \"Russian\": 0.7525743707093822,\n","    \"Chinese\": 0.6132518796992481\n","  }\n","}\n","\n","Saving analysis results...\n","  AUC results saved successfully to: /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/equity_auc_results.pkl\n","  ROC results saved successfully to: /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/equity_roc_results.pkl\n","\n","============================================================\n","=== PART 4: SURVIVAL ANALYSIS PREPARATION ===\n","============================================================\n","Calculating time-to-event ('duration_30day') for survival analysis...\n","  Created 'duration_30day' column.\n","  Sample of duration and mortality:\n","               intime        dod  mortality_30day  duration_30day\n","0 2131-01-11 04:20:05 2131-01-20                1        8.819387\n","1 2160-05-18 10:00:53        NaT                0       30.000000\n","2 2131-03-09 21:33:00 2131-03-10                1        0.102083\n","3 2129-08-04 12:45:00 2131-03-10                0       30.000000\n","4 2130-09-24 00:50:00 2131-03-10                0       30.000000\n","\n","  Summary statistics for 'duration_30day':\n","count    32899.000000\n","mean        26.002275\n","std          8.826397\n","min          0.001000\n","25%         30.000000\n","50%         30.000000\n","75%         30.000000\n","max         30.000000\n","Name: duration_30day, dtype: float64\n","\n","============================================================\n","=== PART 5: SURVIVAL ANALYSIS (Kaplan-Meier & Cox) ===\n","============================================================\n","\n","--- Kaplan-Meier Survival Analysis ---\n","\n","  Generating KM plot stratified by 'ethnicity'...\n","    Log-rank test (ASIAN vs ASIAN - CHINESE): p-value = 0.8500\n","    Log-rank test (ASIAN vs BLACK/AFRICAN AMERICAN): p-value = 0.0403\n","    Log-rank test (ASIAN vs HISPANIC/LATINO - PUERTO RICAN): p-value = 0.0214\n","    Log-rank test (ASIAN vs OTHER): p-value = 0.0013\n","    Log-rank test (ASIAN vs UNABLE TO OBTAIN): p-value = 0.4180\n","    Log-rank test (ASIAN vs UNKNOWN): p-value = 0.0005\n","    Log-rank test (ASIAN vs WHITE): p-value = 0.0229\n","    Log-rank test (ASIAN vs WHITE - OTHER EUROPEAN): p-value = 0.1669\n","    Log-rank test (ASIAN vs WHITE - RUSSIAN): p-value = 0.0847\n","  KM plot saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/km_plot_ethnicity.png\n","\n","  Generating KM plot stratified by 'insurance'...\n","    Log-rank test (Medicaid vs Medicare): p-value = 0.0000\n","    Skipping KM for category 'OTHER' in factor 'insurance' (too few samples: 1)\n","    Log-rank test (Medicaid vs Other): p-value = 0.0186\n","    Log-rank test (Medicaid vs Private): p-value = 0.0000\n","    Log-rank test (Medicaid vs UNKNOWN): p-value = 0.0000\n","  KM plot saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/km_plot_insurance.png\n","\n","  Generating KM plot stratified by 'language'...\n","    Log-rank test (Chinese vs English): p-value = 0.0025\n","    Log-rank test (Chinese vs OTHER): p-value = 0.0119\n","    Log-rank test (Chinese vs Russian): p-value = 0.4068\n","    Log-rank test (Chinese vs Spanish): p-value = 0.0025\n","    Log-rank test (Chinese vs UNKNOWN): p-value = 0.0007\n","  KM plot saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/km_plot_language.png\n","\n","--- Cox Proportional Hazards Analysis ---\n","  Preparing data for Cox model (dummy variables)...\n","  Data prepared for Cox model with 25 columns.\n","\n","  Fitting Cox Main Effects Model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/lifelines/utils/__init__.py:1100: ConvergenceWarning: Column(s) ['insurance_OTHER'] have very low variance. This may harm convergence. 1) Are you using formula's? Did you mean to add '-1' to the end. 2) Try dropping this redundant column before fitting if convergence fails.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n"]},{"output_type":"stream","name":"stdout","text":["  Main Effects Model Summary:\n","<lifelines.CoxPHFitter: fitted with 32899 total observations, 25882 right-censored observations>\n","             duration col = 'duration_30day'\n","                event col = 'mortality_30day'\n","                penalizer = 0.01\n","                 l1 ratio = 0.0\n","      baseline estimation = breslow\n","   number of observations = 32899\n","number of events observed = 7017\n","   partial log-likelihood = -69725.850\n","         time fit was run = 2025-04-27 12:56:20 UTC\n","\n","---\n","                                           coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n","covariate                                                                                                                                  \n","age                                       0.011     1.011     0.001           0.009           0.013               1.009               1.013\n","sofa                                      0.161     1.175     0.003           0.155           0.167               1.168               1.182\n","charlson_comorbidity_index                0.125     1.134     0.004           0.117           0.134               1.124               1.143\n","gender_M                                 -0.149     0.862     0.024          -0.196          -0.102               0.822               0.903\n","ethnicity_ASIAN___CHINESE                -0.025     0.975     0.136          -0.292           0.243               0.746               1.275\n","ethnicity_BLACK_AFRICAN_AMERICAN         -0.187     0.830     0.073          -0.331          -0.043               0.719               0.958\n","ethnicity_HISPANIC_LATINO___PUERTO_RICAN -0.144     0.866     0.133          -0.406           0.117               0.666               1.124\n","ethnicity_OTHER                          -0.135     0.874     0.073          -0.279           0.009               0.757               1.009\n","ethnicity_UNABLE_TO_OBTAIN                0.265     1.303     0.109           0.050           0.479               1.051               1.615\n","ethnicity_UNKNOWN                         0.443     1.557     0.069           0.307           0.578               1.360               1.783\n","ethnicity_WHITE                          -0.080     0.923     0.065          -0.207           0.047               0.813               1.048\n","ethnicity_WHITE___OTHER_EUROPEAN         -0.077     0.926     0.108          -0.290           0.136               0.748               1.145\n","ethnicity_WHITE___RUSSIAN                -0.065     0.937     0.148          -0.355           0.225               0.701               1.253\n","insurance_Medicare                       -0.085     0.919     0.038          -0.160          -0.010               0.852               0.990\n","insurance_OTHER                           2.865    17.556     1.010           0.885           4.846               2.423             127.196\n","insurance_Other                          -0.176     0.839     0.092          -0.357           0.005               0.700               1.005\n","insurance_Private                        -0.116     0.890     0.042          -0.199          -0.034               0.820               0.967\n","insurance_UNKNOWN                         0.505     1.657     0.096           0.317           0.693               1.374               1.999\n","language_English                         -0.021     0.980     0.095          -0.206           0.165               0.814               1.179\n","language_OTHER                           -0.149     0.861     0.107          -0.359           0.060               0.699               1.062\n","language_Russian                          0.038     1.039     0.143          -0.242           0.319               0.785               1.376\n","language_Spanish                         -0.185     0.831     0.118          -0.416           0.046               0.660               1.047\n","language_UNKNOWN                          0.355     1.426     0.143           0.075           0.635               1.078               1.887\n","\n","                                          cmp to      z       p  -log2(p)\n","covariate                                                                \n","age                                        0.000 10.598 <0.0005    84.758\n","sofa                                       0.000 53.800 <0.0005       inf\n","charlson_comorbidity_index                 0.000 28.452 <0.0005   589.115\n","gender_M                                   0.000 -6.252 <0.0005    31.200\n","ethnicity_ASIAN___CHINESE                  0.000 -0.183   0.855     0.226\n","ethnicity_BLACK_AFRICAN_AMERICAN           0.000 -2.539   0.011     6.491\n","ethnicity_HISPANIC_LATINO___PUERTO_RICAN   0.000 -1.082   0.279     1.840\n","ethnicity_OTHER                            0.000 -1.843   0.065     3.937\n","ethnicity_UNABLE_TO_OBTAIN                 0.000  2.416   0.016     5.994\n","ethnicity_UNKNOWN                          0.000  6.409 <0.0005    32.668\n","ethnicity_WHITE                            0.000 -1.239   0.215     2.215\n","ethnicity_WHITE___OTHER_EUROPEAN           0.000 -0.711   0.477     1.067\n","ethnicity_WHITE___RUSSIAN                  0.000 -0.438   0.661     0.597\n","insurance_Medicare                         0.000 -2.211   0.027     5.210\n","insurance_OTHER                            0.000  2.836   0.005     7.774\n","insurance_Other                            0.000 -1.909   0.056     4.153\n","insurance_Private                          0.000 -2.756   0.006     7.418\n","insurance_UNKNOWN                          0.000  5.276 <0.0005    22.856\n","language_English                           0.000 -0.218   0.827     0.274\n","language_OTHER                             0.000 -1.397   0.162     2.623\n","language_Russian                           0.000  0.269   0.788     0.343\n","language_Spanish                           0.000 -1.573   0.116     3.113\n","language_UNKNOWN                           0.000  2.484   0.013     6.265\n","---\n","Concordance = 0.733\n","Partial AIC = 139497.699\n","log-likelihood ratio test = 4902.847 on 23 df\n","-log2(p) of ll-ratio test = inf\n","\n","  Main Cox model summary saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/cox_model_main_summary.csv\n","\n","  Fitting Cox Interaction Model (SOFA * SDoH)...\n","  Created 19 interaction terms for SOFA.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/lifelines/utils/__init__.py:1100: ConvergenceWarning: Column(s) ['insurance_OTHER'] have very low variance. This may harm convergence. 1) Are you using formula's? Did you mean to add '-1' to the end. 2) Try dropping this redundant column before fitting if convergence fails.\n","\n","  warnings.warn(dedent(warning_text), ConvergenceWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Interaction Model Summary:\n","<lifelines.CoxPHFitter: fitted with 32899 total observations, 25882 right-censored observations>\n","             duration col = 'duration_30day'\n","                event col = 'mortality_30day'\n","                penalizer = 0.01\n","                 l1 ratio = 0.0\n","      baseline estimation = breslow\n","   number of observations = 32899\n","number of events observed = 7017\n","   partial log-likelihood = -69693.946\n","         time fit was run = 2025-04-27 12:56:32 UTC\n","\n","---\n","                                                  coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n","covariate                                                                                                                                         \n","age                                              0.011     1.011     0.001           0.009           0.013               1.009               1.014\n","sofa                                             0.095     1.099     0.011           0.074           0.115               1.077               1.122\n","charlson_comorbidity_index                       0.125     1.133     0.004           0.116           0.133               1.123               1.143\n","gender_M                                        -0.149     0.861     0.024          -0.196          -0.103               0.822               0.903\n","ethnicity_ASIAN___CHINESE                       -0.038     0.962     0.222          -0.474           0.397               0.623               1.487\n","ethnicity_BLACK_AFRICAN_AMERICAN                -0.291     0.747     0.100          -0.486          -0.096               0.615               0.908\n","ethnicity_HISPANIC_LATINO___PUERTO_RICAN        -0.223     0.800     0.230          -0.673           0.227               0.510               1.255\n","ethnicity_OTHER                                 -0.254     0.776     0.098          -0.446          -0.061               0.640               0.941\n","ethnicity_UNABLE_TO_OBTAIN                       0.010     1.010     0.191          -0.363           0.384               0.695               1.468\n","ethnicity_UNKNOWN                                0.426     1.532     0.089           0.252           0.601               1.286               1.824\n","ethnicity_WHITE                                 -0.160     0.852     0.073          -0.302          -0.017               0.739               0.983\n","ethnicity_WHITE___OTHER_EUROPEAN                -0.180     0.836     0.193          -0.557           0.198               0.573               1.219\n","ethnicity_WHITE___RUSSIAN                       -0.097     0.907     0.255          -0.597           0.403               0.550               1.496\n","insurance_Medicare                              -0.046     0.955     0.058          -0.159           0.067               0.853               1.069\n","insurance_OTHER                                  1.453     4.275     7.089         -12.441          15.347               0.000           4.624e+06\n","insurance_Other                                 -0.344     0.709     0.173          -0.684          -0.005               0.504               0.995\n","insurance_Private                               -0.190     0.827     0.068          -0.323          -0.057               0.724               0.945\n","insurance_UNKNOWN                                0.395     1.484     0.191           0.020           0.770               1.020               2.159\n","language_English                                -0.258     0.772     0.106          -0.466          -0.050               0.627               0.951\n","language_OTHER                                  -0.147     0.863     0.143          -0.428           0.134               0.652               1.144\n","language_Russian                                -0.351     0.704     0.215          -0.773           0.071               0.462               1.074\n","language_Spanish                                -0.278     0.758     0.169          -0.608           0.053               0.544               1.055\n","language_UNKNOWN                                 0.102     1.107     0.247          -0.382           0.585               0.683               1.796\n","sofa_x_ethnicity_ASIAN___CHINESE                 0.026     1.027     0.025          -0.022           0.074               0.979               1.077\n","sofa_x_ethnicity_BLACK_AFRICAN_AMERICAN          0.029     1.029     0.011           0.006           0.051               1.006               1.052\n","sofa_x_ethnicity_HISPANIC_LATINO___PUERTO_RICAN  0.024     1.025     0.024          -0.023           0.072               0.977               1.075\n","sofa_x_ethnicity_OTHER                           0.030     1.030     0.011           0.008           0.052               1.008               1.053\n","sofa_x_ethnicity_UNABLE_TO_OBTAIN                0.046     1.047     0.020           0.008           0.085               1.008               1.088\n","sofa_x_ethnicity_UNKNOWN                         0.018     1.018     0.010          -0.002           0.037               0.998               1.037\n","sofa_x_ethnicity_WHITE                           0.025     1.025     0.008           0.009           0.041               1.009               1.042\n","sofa_x_ethnicity_WHITE___OTHER_EUROPEAN          0.029     1.029     0.023          -0.016           0.074               0.984               1.076\n","sofa_x_ethnicity_WHITE___RUSSIAN                 0.012     1.012     0.032          -0.051           0.074               0.951               1.077\n","sofa_x_insurance_Medicare                       -0.004     0.996     0.006          -0.016           0.007               0.984               1.007\n","sofa_x_insurance_OTHER                           0.484     1.623     2.363          -4.147           5.116               0.016             166.599\n","sofa_x_insurance_Other                           0.020     1.020     0.017          -0.014           0.054               0.986               1.055\n","sofa_x_insurance_Private                         0.010     1.010     0.007          -0.004           0.024               0.996               1.025\n","sofa_x_insurance_UNKNOWN                         0.012     1.012     0.019          -0.026           0.050               0.974               1.051\n","sofa_x_language_English                          0.047     1.048     0.010           0.027           0.066               1.028               1.068\n","sofa_x_language_OTHER                            0.017     1.017     0.016          -0.014           0.048               0.986               1.049\n","sofa_x_language_Russian                          0.073     1.076     0.028           0.019           0.128               1.019               1.137\n","sofa_x_language_Spanish                          0.030     1.031     0.018          -0.005           0.065               0.995               1.068\n","sofa_x_language_UNKNOWN                          0.047     1.048     0.024          -0.000           0.095               1.000               1.100\n","\n","                                                 cmp to      z       p  -log2(p)\n","covariate                                                                       \n","age                                               0.000 10.719 <0.0005    86.642\n","sofa                                              0.000  8.981 <0.0005    61.689\n","charlson_comorbidity_index                        0.000 28.272 <0.0005   581.735\n","gender_M                                          0.000 -6.263 <0.0005    31.300\n","ethnicity_ASIAN___CHINESE                         0.000 -0.173   0.863     0.213\n","ethnicity_BLACK_AFRICAN_AMERICAN                  0.000 -2.925   0.003     8.180\n","ethnicity_HISPANIC_LATINO___PUERTO_RICAN          0.000 -0.970   0.332     1.590\n","ethnicity_OTHER                                   0.000 -2.582   0.010     6.668\n","ethnicity_UNABLE_TO_OBTAIN                        0.000  0.054   0.957     0.063\n","ethnicity_UNKNOWN                                 0.000  4.782 <0.0005    19.137\n","ethnicity_WHITE                                   0.000 -2.200   0.028     5.168\n","ethnicity_WHITE___OTHER_EUROPEAN                  0.000 -0.932   0.351     1.510\n","ethnicity_WHITE___RUSSIAN                         0.000 -0.381   0.704     0.507\n","insurance_Medicare                                0.000 -0.804   0.422     1.246\n","insurance_OTHER                                   0.000  0.205   0.838     0.256\n","insurance_Other                                   0.000 -1.986   0.047     4.411\n","insurance_Private                                 0.000 -2.798   0.005     7.603\n","insurance_UNKNOWN                                 0.000  2.063   0.039     4.676\n","language_English                                  0.000 -2.435   0.015     6.069\n","language_OTHER                                    0.000 -1.024   0.306     1.708\n","language_Russian                                  0.000 -1.630   0.103     3.279\n","language_Spanish                                  0.000 -1.645   0.100     3.322\n","language_UNKNOWN                                  0.000  0.412   0.680     0.556\n","sofa_x_ethnicity_ASIAN___CHINESE                  0.000  1.077   0.281     1.830\n","sofa_x_ethnicity_BLACK_AFRICAN_AMERICAN           0.000  2.503   0.012     6.344\n","sofa_x_ethnicity_HISPANIC_LATINO___PUERTO_RICAN   0.000  0.998   0.318     1.652\n","sofa_x_ethnicity_OTHER                            0.000  2.666   0.008     7.025\n","sofa_x_ethnicity_UNABLE_TO_OBTAIN                 0.000  2.365   0.018     5.795\n","sofa_x_ethnicity_UNKNOWN                          0.000  1.805   0.071     3.815\n","sofa_x_ethnicity_WHITE                            0.000  3.017   0.003     8.611\n","sofa_x_ethnicity_WHITE___OTHER_EUROPEAN           0.000  1.256   0.209     2.257\n","sofa_x_ethnicity_WHITE___RUSSIAN                  0.000  0.365   0.715     0.483\n","sofa_x_insurance_Medicare                         0.000 -0.746   0.456     1.134\n","sofa_x_insurance_OTHER                            0.000  0.205   0.838     0.256\n","sofa_x_insurance_Other                            0.000  1.164   0.245     2.032\n","sofa_x_insurance_Private                          0.000  1.378   0.168     2.573\n","sofa_x_insurance_UNKNOWN                          0.000  0.620   0.535     0.901\n","sofa_x_language_English                           0.000  4.719 <0.0005    18.684\n","sofa_x_language_OTHER                             0.000  1.078   0.281     1.831\n","sofa_x_language_Russian                           0.000  2.626   0.009     6.855\n","sofa_x_language_Spanish                           0.000  1.681   0.093     3.430\n","sofa_x_language_UNKNOWN                           0.000  1.942   0.052     4.261\n","---\n","Concordance = 0.733\n","Partial AIC = 139471.893\n","log-likelihood ratio test = 4966.653 on 42 df\n","-log2(p) of ll-ratio test = inf\n","\n","  Interaction Cox model summary saved to /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/cox_model_interaction_summary.csv\n","\n","Survival analysis results (plot paths, Cox summaries) saved to: /content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/survival_analysis_results.pkl\n","\n","============================================================\n","=== SCRIPT EXECUTION COMPLETE ===\n","============================================================\n","Check the directory '/content/drive/MyDrive/DukeDatathon_2025_Team3/AnalysisResults/' for analysis outputs.\n"]}]}]}